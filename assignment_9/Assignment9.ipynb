{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xwZKbE3vFwP"
   },
   "source": [
    "# Assignment 9\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python files. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v6OVU_HvH9k"
   },
   "source": [
    "# Exercise 1: Text Classification (10 points)\n",
    "\n",
    "Based on your implementation of the `Corpus` and `Document` classes from the previous assignment, you will now build a simple Naive Bayes classifier to classify each document in the test section of the Reuters News corpus. \n",
    "\n",
    "We will use the TF-IDF metric as the feature for our classifier. TF-IDF of a term $t$ in a document $d$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{TF-IDF}(t,d) = \\frac{\n",
    "    \\text{TF}(t,d)\n",
    "  }{\n",
    "    \\text{IDF}(t)\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "with $\\text{TF}(t,d)$ being the defined as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{TF}(t,d) = \\frac{\n",
    "    f_{t,d}\n",
    "  }{\n",
    "    \\sum_{t'} f_{t',d}\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "where $f_{t,d}$ is the absolute frequency of term $t$ in document $d$.\n",
    "\n",
    "and $\\text{IDF}(t)$ being defined as \n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{IDF}(t) = \\frac{\n",
    "    N\n",
    "  }{\n",
    "    |\\{d \\in D: C_d(t) > 0\\}|\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ stands for the documents in the corpus, $N=|D|$ and $C_d(t)$ is the number of times term $t$ occurs in document $d$.\n",
    "\n",
    "In a TF-IDF matrix, documents are represented by the rows of the matrix and TF-IDF features by its columns. This means that each row vector consists of the TF-IDF value for a term taken from a fixed, shared vocabulary given the document, i. e. $\\text{TF-IDF}(t,d)$, for $t \\in V$ ([this](https://www.researchgate.net/profile/Maryam-Hourali/publication/306358542/figure/tbl1/AS:648973966651395@1531738859631/Some-Part-of-TF-IDF-Term-Document-Matrix.png) is a small example). \n",
    "\n",
    "## 1.1 Vocabulary as feature space (2 points)\n",
    "\n",
    "Construct a shared vocabulary $V$ for the Reuters corpus, using both the train set and the test set. You are expected to reduce the size of the vocabulary by \n",
    "  * Preprocessing (removing punctuation, lowercasing, tokenizing). (0.25 points)\n",
    "  * Lemmatizing the tokenized text. (0.5 points)\n",
    "  * Setting a $\\text{min_df}$ and $\\text{max_df}$ and removing all terms from the vocabulary that occur in less then $\\text{min_df}$ and more than $\\text{max_df}$ documents. You should support your choice with a source from the internet or your own reasoning. (0.5 point)\n",
    "  * Why is it necessary to reduce the size of the vocabulary and to set a lower and upper bound to document frequency? Explain in 2-3 sentences. (0.25 points)\n",
    "\n",
    "You are allowed to use any Python package useful to the task. We suggest using NLTK's [RegexpTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer) for tokenization and [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer) for lemmatization. The implementation should be in the `reduce_vocabulary` method of the `Corpus` class. Check that your implementation is correct by executing the code cell below and comparing vocabulary sizes before and after the reduction. \n",
    "  \n",
    "As always, you are free to define new methods as you need them.  \n",
    "\n",
    "**Answers**\n",
    "- \n",
    "- It is important to reduce the vocabulary size in order to preserve only words that provide valuable information for the task. Words that appear in many or all documents are not helpful for distinguishing between different document categories. On the other hand, words that appear only in a few documents (e.g. 1 or 2) are probably not representative features of that document category. Lastly, due to computational limitations, it might be required to reduce the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8sYqeIW08JZg"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pavle/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "from nltk.corpus import reuters, stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "FvJYIFZM6LQg"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 47/10788 [00:00<00:24, 444.28it/s]Loading Reuters corpus...\n",
      "100%|██████████| 10788/10788 [00:14<00:00, 744.68it/s]\n",
      "\n",
      "Vocab size before reduction: 28371\n",
      "\n",
      "Vocab size after reduction: 6236\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading Reuters corpus...\")\n",
    "corpus = exercise_1.Corpus(\n",
    "    documents=[\n",
    "    exercise_1.Document(fileid, reuters.raw(fileid), reuters.categories(fileid), stop_words=stop_words) \n",
    "    for fileid in tqdm(reuters.fileids())],\n",
    "    categories=reuters.categories()\n",
    ")\n",
    "print(\"\\nVocab size before reduction:\", len(corpus.terms()))\n",
    "\n",
    "# TODO: set min_df, max_df\n",
    "min_df = 3\n",
    "max_df = 7\n",
    "\n",
    "reduced_vocab = corpus.reduce_vocab(min_df=min_df, max_df=max_df)\n",
    "\n",
    "print(\"\\nVocab size after reduction:\", len(reduced_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ztKAsZ-HTV"
   },
   "source": [
    "## 1.2 TF-IDF matrix (2 points)\n",
    "\n",
    "1. Implement the method `_idfs` of the `Corpus` class. It should take the reduced vocabulary as input and return a dictionary containing the IDFs of each word in the reduced vocabulary. Print the IDFs of the first 10 terms (sorted lexicographically) from the reduced vocabulary. Store the IDFs in a class variable `idfs`. Why is it a good idea to calculate IDFs first? (1 points)\n",
    "\n",
    "2. Implement the method `_tfs_idfs` of the corpus class. It should return a vector or a a list containing the TF-IDFs of all terms in the reduced vocabulary for a single document. It should use the `_idfs` method once internally. (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gqWAqjjvFD-",
    "outputId": "b7be5b09-6cea-4498-fbab-a14c64434ee9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimating idfs\n0 8.123493975903614\n00 51.86538461538461\n000 3.546351084812623\n007 980.7272727272727\n009 1078.8\n010 634.5882352941177\n02 105.76470588235294\n04 96.32142857142857\n040 469.04347826086956\n05 75.97183098591549\n"
     ]
    }
   ],
   "source": [
    "# TODO: load and print IDFs!\n",
    "idfs = corpus._idfs(reduced_vocab)\n",
    "print(\"Estimating idfs\")\n",
    "for term in reduced_vocab[:10]:\n",
    "  print(term, idfs[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z7TVYQyAgNW"
   },
   "source": [
    "## 1.3 Train/test split (1.5 points)\n",
    "\n",
    "1. Implement the method `_category2index`. It should take a string (the name of the category) as input and return its index in the `Corpus`-internal list of categories. (0.25 points)\n",
    "2. Implement the method `compile_dataset` of the `Corpus` class. It should take the reduced vocabulary as input and return two tuples: (train TF-IDF matrix, train labels) and (test TF-IDF matrix, test labels). The train matrix/labels should be derived from the train section of the Reuters dataset (file-ids starting with `training/`) and the test matrix/labels from the test section (file-ids starting with `test/`).\n",
    "\n",
    "  Make use of the methods `_tf_idfs` and `_category2index` (1 point)\n",
    "\n",
    "3. Use the method `compile_dataset` to load the train and test data into variables. Please name the variables such that we can distinguish the train data from the test data. Show the size of the train and test set. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Dv0gDKDWC2Dp",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data shape: (7769, 6236), train labels shape: (7769,)\nTest data shape: (3019, 6236), test labels: (3019,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: load train and test data\n",
    "(X_train, Y_train), (X_test, Y_test) = corpus.compile_dataset(reduced_vocab)\n",
    "\n",
    "# Show size of train and test set\n",
    "print(f\"Train data shape: {X_train.shape}, train labels shape: {Y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, test labels: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC4liSllDOVQ"
   },
   "source": [
    "## 1.4: Naive Bayes Classifier (5 points)\n",
    "\n",
    "A Naive Bayes classifier assigns a datapoint $x = x_1,...x_n$ to a class $C_k$ ($1 \\leq k \\leq K$, with $K$ being the number of classes) with probability $P(C_k|x)$ given by:\n",
    "\n",
    "\\begin{equation}\n",
    "  p(C_k|x) = \\frac{\n",
    "    p(C_k)p(x|C_k)\n",
    "  }{\n",
    "    p(x)\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "1.  Describe the idea behind Naive Bayes in 3-4 sentences. Do so by explaining the terms 'naive' and 'Bayes(ian)' (1 point)\n",
    "2. For each part of the above formula, assign it to one of the following categories, and give a short explanation. (1 point)\n",
    "  * Prior\n",
    "  * Posterior\n",
    "  * Likelihood\n",
    "  * Evidence\n",
    "\n",
    "3. In our dataset from 1.3, what corresponds to $C_k$? What to $x$? (0.5 points)\n",
    "\n",
    "4. What is a good baseline for estimating the accuracy of our classifier? How would you evaluate it? Explain in 1-2 sentences **and** support your answer with code. This will also help you check the accuracy you get on the actual data. (1 point)\n",
    "\n",
    "5. Train a Naive Bayes classifier on the train section of our dataset and report precision, accuracy and F-score on the test section. You may use the class [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) and the method [precision_recall_fscore](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) from the [scikit-learn](https://scikit-learn.org/stable/install.html) Python package. You can write the code in the code cell below. (2 points)\n",
    "  \n",
    "6. Do you observe a difference in the F-scores of different classes? Why? What could you do to account for your finding? (0.5 points)\n",
    "\n",
    "**Answers** <br>\n",
    "1. Naive Bayes is collection of algorithms based on Bayes Theorem which assume that all features are independent of each other and they contribute equally to classification. Bayesian approach is a probabilistic approach for updating a prior belief for values given that some event happend. Naive part comes from independence assumption which allows for transforming the formula $P(C_k|x) = \\frac{P(C_k)P(x|C_k)}{P(x)}$ to $P(C_k|x) = \\frac{P(C_k)P(x_1|C_k)P(x_2|C_k)...P(x_n|C_k)}{P(x_1)P(x_2)...P(x_n)}$. Finally, since $P(x_1)P(x_2)...P(x_n)$ is constant over training and different classes, it can be omitted, and leaving the final simplified formula $P(C_k|x) = P(C_k) \\prod_{i=1}^n{P(x_i|C_k)}$. <br>\n",
    "2. Parts of the formula: <br>\n",
    "  * Prior - $P(C_k)$ -> It is initial belief for the value of interest.\n",
    "  * Posterior - $P(C_k|x)$ -> It is updated belief for the value of interest given that some event happend.\n",
    "  * Likelihood - $P(x|C_k)$ -> It is probability that some event will happen given the value of interest.\n",
    "  * Evidence - $P(x)$ -> It is probability that some event that cause update of the value of interest happend.\n",
    "3. $C_k$ is $k^{th}$ category of all posible for documents classification, while $x$ is feature vectors, i.e. reduced vocabulary. <br>\n",
    "4. <br>\n",
    "6. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVMp0gIzIi0H"
   },
   "outputs": [],
   "source": [
    "# TODO: Find accuracy of baseline classifier\n",
    "\n",
    "# TODO: train classifier, report precision, recall, fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StlojTryeIdd"
   },
   "source": [
    "## Bonus: Support Vector Machines (1.5 points)\n",
    "\n",
    "Consider the task of Named Entity Recognition. In a simplified scenario, you want to decide for each word if it belongs to one of the following classes: {not-named-entity, person, city, country, currency}. An expert in the field tells you that you should start with the following set of features:\n",
    "- is the whole word in capitals\n",
    "- is the first letter capitalized\n",
    "- does it begin a sentence\n",
    "- number of characters \n",
    "- is a stopword\n",
    "- number of Wikipedia articles that contain this word in their title\n",
    "\n",
    "1. Come up with at least 3 more features for this problem. (0.2 points)\n",
    "2. How can we numerically represent each datapoint? What is the mathematical object called and what is the set in which it lives? (0.2 points)\n",
    "3. What is a hyperplane and how can it be used in this context? (0.2 points)\n",
    "4. Imagine that you've been given two features: $f_1, f_2$ and the following dataset. The task is currently only to distinguish between two classes. Draw the points and 3 hyperplanes:\n",
    "  - one that mispredicts at least one datapoint\n",
    "  - one that predicts everything correctly\n",
    "  - one that predicts everything correctly but is in some sense worse than the previous one\n",
    "\n",
    "|Data point|$f_1$|$f_2$|class|\n",
    "|---|---|---|---|\n",
    "|$d_1$|2|2|Y|\n",
    "|$d_2$|10|9|Y|\n",
    "|$d_3$|2|5|Y|\n",
    "|$d_4$|3|5|Y|\n",
    "|$d_5$|2|-2|N|\n",
    "|$d_6$|10|0|N|\n",
    "|$d_7$|10|-4|N|\n",
    "|$d_8$|3|3|N|\n",
    "\n",
    "  In all cases provide the formula for the hyperplane and explain how to use it to make a decision regarding which class it belongs to. (0.65 points)\n",
    "\n",
    "5. In the previous question, you created hyperplanes that helped you in determining which of the two classes the datapoint belongs to. How would you extend this to solve the original problem, i.e. predicting which of the 5 classes the datapoint belongs to? (0.25 points)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TH0UZYLl2Tp2",
    "K6a2UUP09Mh0"
   ],
   "name": "Assignment9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}