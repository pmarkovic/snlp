{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btGJ6nRrkaAn"
   },
   "source": [
    "# SNLP Assignment 4\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python file for exercise 2. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqaltMXrk082"
   },
   "source": [
    "## Exercise 1: Encodings (3 points)\n",
    "\n",
    "#### 1.1 (0.5 points)\n",
    "\n",
    "What are the benefits of having a fixed-width code as opposed to variable-width? e.g. `0011` for `e` and `1111001010` for `Z` (since `e` is much more common than `Z`, this would save some space in variable length encoding).\n",
    "\n",
    "Imagine that you are given a string in UTF16 (variable-width character encoding). What issues can you encounter? Provide a specific engineering example. \n",
    "\n",
    "#### 1.2 (1 point)\n",
    "\n",
    "The ASCII encoding uses 8 bits per single character. The target alphabet in this case is binary: $\\{0, 1\\}$. Given a text of length $n$, the encoding takes $n\\times 8$ bits in memory (without taking alignment into consideration).\n",
    "\n",
    "- How would you adapt the ASCII encoding if the target alphabet had three symbols $\\{0,1,2\\}$?\n",
    "- How many \"trits\" (number of symbols from  {0,1,2}) would it take to encode $n$ characters in this new system? Be precise in your mathematical explanation, and be careful which way you round and beware of $\\pm 1$ errors.\n",
    "- The UTF32 encoding uses 32 bits per single character. How does your computation change?\n",
    "\n",
    "\n",
    "#### 1.3 (1.5 points)\n",
    "\n",
    "Given the following text, construct a [Huffman tree](https://en.wikipedia.org/wiki/Huffman_coding) and encode it (at character level).\n",
    "\n",
    "- Show the final tree (ideally as a diagram together with frequency counts). You can work it out on a paper and include a photograph.\n",
    "- Show the encoded sequence (ideally in comparison to ASCII encoded sequence in binary).\n",
    "- Compare the length to ASCII encoding (n*8 characters). What improvement did you achieve? Could you expect the same amount of improvement on the entirety of Wikipedia?\n",
    "\n",
    "```\n",
    "Three thousand three hundred and thirty-three silver syringes.\n",
    "```\n",
    "\n",
    "Treat space as a character as well and `t` and `T` as two different characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "1.1 In variable-width code, a word that contains frequently-occuring characters will have a shorter representation. In contrast, the encoding of a word with infrequently-used letters would be significantly larger in size. In this case where a word can contain any character, fixed-width encoding would work better since all characters would be represented by the same number of bits. With a UTF-16 encoded string, compatibility issues with Byte Order Mark (BOM) may arise. BOM is a sequence of unprintable Unicode bytes, placed at the beginning of the text, which makes it easier for compatible applications to determine the subtype of Unicode format and to define the direction for reading the bytes. This often causes compatibility issues because not all applications know how to handle the BOM. For non-compatible applications, this sequence of bytes is considered as some normal characters in extended ASCII. \n",
    "[Source](https://www.invivoo.com/en/how-to-solve-unicode-encoding-issues/)\n",
    "\n",
    "1.2   \n",
    "* ASCII encoding with 8 bits per single character supports 256 code words. If the target alphabet has three symbols $\\{0,1,2\\}$, the optimal length of the code words would be changed, as per the equation $l_i = -\\log_D p(w_i)$. $l_i = -\\log_3 \\frac{1}{256}$ = 5.047 = ~5. This means, with a target alphabet of 3 symbols, 5 bits will be needed to uniquely represent each code word and the encoding would now support 3⁵=243 unique code words. \n",
    "* Given a text of length $n$, the encoding will now take $n\\times 5$ bits in memory.\n",
    "* With UTF32 encoding, $n\\times 32$ bits will be used to encode $n$ characters.\n",
    "\n",
    "1.3\n",
    "Huffman encoding:\n",
    "\n",
    "| char  | freq | code       | code-length |\n",
    "|-------|------|------------|-------------|\n",
    "| e     | 9    | 00         | 2           |\n",
    "| r     | 7    | 01         | 2           |\n",
    "| space | 7    | 100        | 3           |\n",
    "| h     | 6    | 101        | 3           |\n",
    "| t     | 5    | 1100       | 4           |\n",
    "| s     | 4    | 1101       | 4           |\n",
    "| n     | 4    | 11100      | 5           |\n",
    "| d     | 4    | 11101      | 5           |\n",
    "| i     | 3    | 111100     | 6           |\n",
    "| y     | 2    | 111101     | 6           |\n",
    "| a     | 2    | 1111100    | 7           |\n",
    "| u     | 2    | 1111101    | 7           |\n",
    "| l     | 1    | 11111100   | 8           |\n",
    "| o     | 1    | 11111101   | 8           |\n",
    "| v     | 1    | 111111100  | 9           |\n",
    "| g     | 1    | 111111101  | 9           |\n",
    "| -     | 1    | 111111110  | 9           |\n",
    "| T     | 1    | 1111111110 | 10          |\n",
    "| .     | 1    | 1111111111 | 10          |\n",
    "\n",
    "\n",
    "Three thousand three hundred and thirty-three silver syringes.\n",
    "* Huffman encoded sequence:\n",
    "<!--- 1111111110.101.01.00.00.100.1100.101.11111101.1111101.1101.1111100.11100.11101.100.1100.101.01.00.00.100.101.1111101.11100.11101.01.00.11101.100.1111100.11100.11101.100.1100.101.111100.01.1100.111101.111111110.1100.101.01.00.00.100.1101.111100.11111100.111111100.00.01.100.1101.111101.01.111100.11100.111111101.00.1101.1111111111\n",
    "-->\n",
    "1111111110101010000100110010111111101111110111011111100111001110110011001010100001001011111101111001110101001110110011111001110011101100110010111110001110011110111111111011001010100001001101111100111111001111111000001100110111110101111100111001111111010011011111111111\n",
    "\n",
    "ASCII-encoding : TODO\n",
    "\n",
    "| char  | freq | code     |\n",
    "|-------|------|----------|\n",
    "| e     | 9    | 00000000 |\n",
    "| r     | 7    | 00000001 |\n",
    "| space | 7    | 00000010 |\n",
    "| h     | 6    | 00000011 |\n",
    "| t     | 5    | 00000100 |\n",
    "| s     | 4    | 00000101 |\n",
    "| n     | 4    | 00000110 |\n",
    "| d     | 4    | 00000111 |\n",
    "| i     | 3    | 00001000 |\n",
    "| y     | 2    | 00001001 |\n",
    "| a     | 2    | 00001010 |\n",
    "| u     | 2    | 00001011 |\n",
    "| l     | 1    | 00001100 |\n",
    "| o     | 1    | 00001101 |\n",
    "| v     | 1    | 00001110 |\n",
    "| g     | 1    | 00001111 |\n",
    "| -     | 1    | 00010000 |\n",
    "| T     | 1    | 00010001 |\n",
    "| .     | 1    | 00010010 |\n",
    "\n",
    "ASCII-encoded sequence: Three thousand three hundred and thirty-three silver syringes.\n",
    "\n",
    "* ASCII encoding takes 62*8=496 bits while huffman encoding for the same sequence takes 248 bits. There is an improvement of around 50%. On a wikipidea article, there would not be such a significant improvement because a wikipedia artcile will typically have more unique characters. This means, ASCII encoding of 8 bits would be better utilized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYHsaBBBIAgb"
   },
   "source": [
    "## Exercise 2: Conditional Entropy of DNA (7 points)\n",
    "In this exercise, we will see how conditional entropy is calculated for genome sequences. Read the instructions given below carefully.\n",
    "\n",
    "### 2.0 Getting started with biopython\n",
    "\n",
    "1. **Installing biopython** <br/>\n",
    "Install [biopython](https://biopython.org/) to your local Python environment. Dependencies should be installed automatically. <br/>\n",
    "The installation instructions for biopython are given in the attached link. A simple pip installation does the trick. \n",
    "2. **Saving data files**  <br/>\n",
    "Download the genome of *Drosophila melanogaster* from [kaggle](https://www.kaggle.com/mylesoneill/drosophila-melanogaster-genome?select=genome.fa). For this, you wil need to create an account on kaggle. <br/> \n",
    "To save the data files you download, create a folder called `data` and save the csv and fasta files directly in this folder, since the same path will be used for retrieving the relevant files, as you can see below in the provided code. Conversely, you can simply rename the `archive` folder as `data`. \n",
    "3. **How to read a fasta file**  <br/>\n",
    "As your first task, you have to sample a reduced version of the genome using the pre-implemented `sample_records` function in `exercise_2.py`. The function will write the reduced genome to a [fasta](https://en.wikipedia.org/wiki/FASTA_format) file. <br/>\n",
    "Biopython can be imported in your Python file with the statement `import Bio`. \n",
    "For reading fasta files, look up how to utilise biopython's SeqIO module for parsing the file structure to read genome information in the form of sequences. The result should be akin to reading a list of sentences from a text file. Check the `sample_records` function for further guidance. \n",
    "4. **Understanding genome sequences** <br/>\n",
    "Parsing the fasta file results in an [iterator](https://wiki.python.org/moin/Iterator) of sequence records for the genomes in a class called SeqRecord. <br/>\n",
    "Structure: *SeqRecord* <br/>\n",
    "Elements: <br/>\n",
    "o Meta-features of the sequence (*Name*, *Id*, *Description*, *Number of features*) <br/>\n",
    "o The actual sequence of nucleotides (*Seq*) <br/>\n",
    "These sequences comprise chains formed of the same basic building blocks - the nucleotides A, G, C, T. You will observe that these sequences are case sensitive i.e. they contain both uppercase and lowercase chains. We will explicitly tell you what they mean and how to handle them over the course of this exercise. <br/>\n",
    "5. **Processing genome sequences** <br/>\n",
    "We can deal with the obtained sequences as if they were normal Python strings. Your goal is to first combine all these sequences into a continuous string of nucleotides, and then extract k-mers from this final \"text\" corpus.\n",
    "5. **Difference between k-mers and n-grams** <br/>\n",
    "For all intents and purposes, k-mers are to genome sequences what n-grams are to word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting biopython\n",
      "  Downloading biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/rrichajalota/.local/lib/python3.6/site-packages (from biopython) (1.19.4)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.78\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FoJB53XTzqe-"
   },
   "outputs": [],
   "source": [
    "# 2.0 Sample a reduced version of the genome\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)\n",
    "\n",
    "N = 100\n",
    "\n",
    "genome_loc = Path(\"./data/genome.fa\")\n",
    "genome_red_loc = Path(\"./data/genome_reduced.fa\")\n",
    "\n",
    "exercise_2.sample_records(genome_loc, genome_red_loc, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTnny1dB2f1s"
   },
   "source": [
    "### 2.1: K-mers (0.5 points) \n",
    "\n",
    "[k-mers](https://en.wikipedia.org/wiki/K-mer) are sequences of nucleotides of length $k$. You can see them as the DNA variant of n-grams, only with nucleotides as its \"words\".\n",
    "\n",
    "Implement the function `get_k_mers` that assembles sequences of nucleotides from the fasta file produced in 2.0. For now, convert all characters to uppercase.\n",
    "\n",
    "e. g. for a sequence\n",
    "```\n",
    "GTAGAGCTGT\n",
    "```\n",
    "The 2-mers to be sampled are, just as in a bigram language model:\n",
    "```\n",
    "GT, TA, AG, GA, AG, GC, CT, TG, GT\n",
    "```\n",
    "This example is taken from the Wikipedia article, see there for higher $k$. \n",
    "\n",
    "Now, show the output of the function for $k = 2$, i. e. all 2-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6RHI1gSm40TX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: chrUn_DS485863v1\n",
      "Name: chrUn_DS485863v1\n",
      "Description: chrUn_DS485863v1\n",
      "Number of features: 0\n",
      "Seq('cctgcatataggaaaatgtctgctggaaatcttttagtactattatgtttagtt...caa')\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 2.1 get k-mers\n",
    "k_mers = exercise_2.get_k_mers(genome_red_loc, 2)\n",
    "print(k_mers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWoEA5bG6Ig0"
   },
   "source": [
    "### 2.2 K-mer language models (3 points)\n",
    "\n",
    "*  Implement the function `k_mer_statistics`. It should estimate $K$ language models with k-mer sizes $1,...,K$, and return two probability distributions: <br/>\n",
    "a) relative k-mer frequencies and <br/>\n",
    "b) conditional k-mer probabilities. <br/>\n",
    "As always, you may modify the function signature to suit your needs (but you must comment on the changes you make). (2.5 points) <br/>\n",
    "\n",
    " **Hint:** Calculate the relative frequencies up to K, and then use these for calculating the $k^{th}$ level conditional probabilities using the $(k-1)^{th}$ level relative frequencies. (Refer to the formulae you obtained in Assignment 2 Ex. 2.1)\n",
    "* Plot the probabilites of all k-mers for $K=5$ language models vs their rank, and use the log-log scale on the axes. Do so by implementing the function `plot_k_mers`. Does the curve look similar to the one you obtained for natural language? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxnBSEyt-Mtk"
   },
   "outputs": [],
   "source": [
    "# 2.2 k-mer statistics\n",
    "rel_freqs, cond_probs = exercise_2.k_mer_statistics(genome_red_loc, K=5)\n",
    "\n",
    "# plot\n",
    "exercise_2.plot_k_mers(rel_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpcZvXaD_FlR"
   },
   "source": [
    "### 2.3 Conditional entropy (1.5 point)\n",
    "We want to observe how the conditional entropy of our small DNA corpus changes with increasing k-mer size. Conditional Entropy is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "H(W|H) = - \\sum_{i} p(w_i,h_i) \\cdot \\log_2p(w_i|h_i)\n",
    "\\end{equation}\n",
    "\n",
    "Where $p(w_i,h_i)$ is the relative frequency of $(w_i, h_i)$ and $p(w_i|h_i)$ is the conditional probability of $w_i$ given the history $h_i$.  \n",
    "\n",
    "* What do $w_i$, $h_i$ and the probabilities derived from them correspond to in the context of DNA? (0.5 Points)\n",
    "\n",
    "* Using your insights from above, implement the function `conditional_entropy`, that calculates the conditional entropy of a k-mer language model. You may modify the function signature as you wish. (0.5 points)\n",
    "\n",
    "* Estimate up to $K=20$ k-mer language models, calculate their conditional entropies and plot by ascending $k$. What do you observe? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IFuiAn_BHYq"
   },
   "outputs": [],
   "source": [
    "# 2.3 conditional entropy\n",
    "K = 20\n",
    "\n",
    "H_ks = []\n",
    "\n",
    "rel_freqs, cond_probs = exercise_2.k_mer_statistics(genome_red_loc, K=20)\n",
    "\n",
    "for k in range(1, K+1):\n",
    "  H_k = exercise_2.conditional_entropy(rel_freqs[k], cond_probs[k])\n",
    "  print(\"{}-mer cond. entropy is {}\".format(k, H_k))\n",
    "  H_ks.append(H_k)\n",
    "\n",
    "# plot\n",
    "exercise_2.plot_conditional_entropies(H_ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SAfjGH3zT76"
   },
   "source": [
    "### 2.4 Tandem repeats (2 points)\n",
    "\n",
    "A [tandem repeat](https://en.wikipedia.org/wiki/Tandem_repeat) or a mini/microsattelite is a sequence of nucleotides that is repeated multiple times, and the repetitions are immediately adjacent. This example is again taken from the Wikipedia article:\n",
    "\n",
    "```\n",
    "ATTCG ATTCG ATTCG\n",
    "```\n",
    "\n",
    "In the *Drosophila Melanogaster* genome, tandem repeats are represented by lowercase letters, while the non-repeating sequences are in uppercase. Up to 2.3, we ignored tandem repeats and considered all sequences in uppercase. \n",
    "\n",
    "* Read up about tandem repeats, and tell what your expectations about the conditional entropy of tandem repeat regions are as opposed to non-tandem repeat regions (0.5 points)\n",
    "\n",
    "* Implement the functions `get_k_mers_24` and `k_mer_statistics_24` and sample the same language models as in 2.2, but this time exclusively on tandem repeats or non-tandem repeats. This should yield two sets of language models $LM_{TR}$ and $LM_{\\neg TR}$. (1 point)\n",
    "\n",
    "* Calculate the conditional entropies of both $LM_{TR}$ and $LM_{\\neg TR}$. Plot the conditional entropies vs. increasing $K$ as in 2.3. Do you observe any difference? Does it follow your expectation from above? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4GfYb6q4CCl"
   },
   "outputs": [],
   "source": [
    "# 2.4.1 tandem repeats\n",
    "H_ks = []\n",
    "\n",
    "rel_freqs, cond_probs = exercise_2.k_mer_statistics_24(genome_red_loc, K, tandem_repeats=True)\n",
    "\n",
    "for k in range(K):\n",
    "  H_k = exercise_2.conditional_entropy(rel_freqs[k], cond_probs[k])\n",
    "  H_ks.append(H_k)\n",
    "\n",
    "# plot\n",
    "exercise_2.plot_conditional_entropies(H_ks)\n",
    "\n",
    "# 2.4.2 non tandem repeats\n",
    "H_ks = []\n",
    "\n",
    "rel_freqs, cond_probs = exercise_2.k_mer_statistics_24(genome_red_loc, K, tandem_repeats=False)\n",
    "\n",
    "for k in range(K):\n",
    "  H_k = exercise_2.conditional_entropy(rel_freqs[k], cond_probs[k])\n",
    "  H_ks.append(H_k)\n",
    "\n",
    "# plot\n",
    "exercise_2.plot_conditional_entropies(H_ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZChBXzIk18s"
   },
   "source": [
    "## Bonus (1.5 points)\n",
    "\n",
    "The standard Huffman encoding uses a binary target alphabet $\\{0,1\\}$. Assume that you're given a text in alphabet $\\Sigma$ and you compress one input symbol from the alphabet at a time.\n",
    "\n",
    "- Could you adapt the algorithm so that it utilizes three output symbols $\\{0,1,2\\}$?\n",
    "- What about $k$ target symbols ($k < $ alphabet size)?\n",
    "-  What would happen if you used $\\Sigma$ as the output alphabet for Huffman encoding? Would the text remain the same? Would it have the same length?\n",
    "- What changes if the input is words (i.e. sequences from the $\\Sigma$ alphabet with one symbol representing the word boundary)?\n",
    "- What changes if you are allowed to use phrases (i.e. ignore the word boundary) if it helps compression?\n",
    "\n",
    "Answer these questions in 1-2 sentences max."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
