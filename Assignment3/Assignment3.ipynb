{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgfjrzxW3l1W"
   },
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: Rricha Jalota <br/>\n",
    "Student id 1: 7010592 <br/>\n",
    "Email 1: rrja00001 <br/>\n",
    "\n",
    "\n",
    "Name 2: Pavle Markovic <br/>\n",
    "Student id 2: 7007913 <br/>\n",
    "Email 2: pama00002 <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python file for the bonus question (if you attempt it). <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y0Looy74Lor"
   },
   "source": [
    "## Exercise 1: Entropy Intuition (2 points)\n",
    "\n",
    "### 1.1 (0.5 points)\n",
    "\n",
    "Order the following three snippets by entropy (highest to lowest). Justify your answer (view it more intuitively rather than by using a specific character-level language model, though you would probably reach the same conclusion).\n",
    "\n",
    "```\n",
    "1:    A B A A A A B B A A A B A B B B B B A\n",
    "2:    A B A B A B A B A B A B A B A B A B A\n",
    "3:    A B A A A B A B A B A B A B A B A B A\n",
    "```\n",
    "\n",
    "### 1.2 (0.5 point)\n",
    "\n",
    "Words in natural language do not have the maximum entropy given the available alphabet. This creates a redundancy (e.g. the word `maximum` could be uniquely replaced by `mxmm` and everyone would still understand). If the development of natural languages leads to somewhat optimal solutions, why is it beneficial to have such redundancies in communication?\n",
    "\n",
    "If you're uncertain, please refer to this well-written article: [www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html](http://www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html).\n",
    "\n",
    "### 1.3 (1 point)\n",
    "\n",
    "1. Assume you were given a perfect language model that would always assign probability of $1$ to the next word. What would be the cross-entropy on any text? Motivate your answer with formal derivation. (0.5 points)\n",
    "2. How does cross-entropy relate to perplexity? Is there a reason why would one be preferred over the other? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer  \n",
    "1.1  \n",
    "**1:    A B A A A A B B A A A B A B B B B B A** - A is either followed by another A or B. Likewise, B can be followed by another B or an A. Therefore, difficult to predict the next character correctly and hence, this sequence carries maximum surprise quotient.   \n",
    "**3:    A B A A A B A B A B A B A B A B A B A** - A is either followed by another A or B, but B is always followed by A.  \n",
    "**2:    A B A B A B A B A B A B A B A B A B A** - follows a pattern in which B always comes after A and A comes after B, therefore least degree of surprisal \n",
    " \n",
    "1.2 Redundancies in natural language help in reducing the uncertaining of an utterance. The more information we have the less uncertain we are about speaker's intent. Since, the goal of effective communication is accurately delivering a piece of information with minimal disambiguity, redundancy helps in achieving this.  \n",
    "\n",
    "1.3  \n",
    "1. A perfect language model that always assigns probability of  1  to the next word would have a cross entropy of 0. <br> Cross Entropy, $ H(L,M) = { \\sum^n_1 p(w_i) \\log p(w_i|w_{i-1})}$ if $p(w_i|w_{i-1}) = 1$, $\\log p(w_i|w_{i-1}) = 0$, thus $H(L,M)= 0$.  \n",
    "2. Cross Entropy = $\\log(Perplexity)$  \n",
    "Cross Entropy uses logarithms, which is computationally easier to calculate.  However, due to exponentiation, improvements in perplexity turn out to be more substantial than the equivalent improvement in entropy. Moreover, the evaluation metric is brought back to linear scale which is easier for humans to understand. Therefore, perplexity is used more often.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "## Exercise 2: Harry Potter and the Measure of Uncertainty (4 points)\n",
    "\n",
    "#### 2.1 (2.5 points)\n",
    "\n",
    "Harry, Hermione, and Ron are trying to save the Philosopher's Stone. To do this, they have to cross a series of hurdles to reach the room where the stone is kept. Currently, they are trapped in a chamber whose exit is blocked by fire. On a table before them are 7 potions.\n",
    "\n",
    "|P1|P2|P3|P4|P5|P6|P7|\n",
    "|---|---|---|---|---|---|---|\n",
    "\n",
    "Of these, 6 potions are poisons and only one is the antidote that will get them through the exit. Drinking the poison will not kill them, but will weaken them considerably. \n",
    "\n",
    "1. There is no way of knowing which potion is a poison and which an antidote. How many potions must they sample *on an average* to pick the antidote? (1 point)\n",
    "\n",
    "Hermione notices a scroll lying near the potions. The scroll contains an intricate riddle written by Professor Snape that will help them determine which potion is the antidote. With the help of the clues provided, Hermione cleverly deduces that each potion can be the antidote with a certain probability. \n",
    "\n",
    "|P1|P2|P3|P4|P5|P6|P7|\n",
    "|---|---|---|---|---|---|---|\n",
    "|1/16|1/4|1/64|1/2|1/64|1/32|1/8|\n",
    "\n",
    "2. In this situation, how many potions must they now sample *on an average* to pick the antidote correctly? (1 point)\n",
    "3. What is the most efficient sequence of potions they must sample to discover the antidote? Why do you claim that in terms of how uncertain you are about guessing right? (0.5 point)\n",
    "\n",
    "#### 2.2 (1.5 points)\n",
    "\n",
    "1. Extend your logic from 2.1 to a Shannon's Game where you have to correctly guess the next word in a sentence. Assume that a word is any possible permutation and combination of 26 letters of the alphabet, and all the words have a length of at most *n*. \n",
    "How many guesses will one have to make to guess the correct word? (1 point) <br/>\n",
    "(**Hint**: Think of how many words can exist in this scenario)\n",
    "\n",
    "2. Why is the entropy lower in real-world languages? How do language models help to reduce the uncertainty of guessing the correct word? (2-3 sentences) (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1\n",
    "1. \n",
    "P(picking an antidote at first guess) = 1/7  \n",
    "P(picking an antidote at second guess) = 6/7 * 1/6 = 1/7    \n",
    "P(picking an antidote at third guess) = 6/7 * 5/6 * 1/5 = 1/7    \n",
    "...   \n",
    "Likewise, P(X=4) = P(X=5) = P(X=6) = P(X=7) = 1/7   \n",
    "$E(x) = \\sum^7_{x=1} x p(x)$ = $p \\sum^7_{x=1}x $   \n",
    "= $1/7 [1 + 2 + 3 + 4 + 5 + 6 + 7] = 28/7 = 4 $   \n",
    "Number of potions must they sample *on an average* to pick the antidote = **4**  \n",
    "<br> \n",
    "\n",
    "2. \n",
    "\n",
    "3. P4 > P2 > P7 > P1 > P6 > P3 == P5\n",
    "\n",
    "##### 2.2\n",
    "1. Considering that the next word could be of any length from 1 to n, we would have to consider all possible words that could be made from 26 alphabets for each of these lengths. This means,   \n",
    "total number of possible words = len_1_word + len_2_word + len_3_word +...+ len_n_word   \n",
    "= $ 26 + (26)² + (26)³ + .... + (26)^n$ = $26 [ 1 + 26 + (26)² + ... + (26)^{n-1}]$ = 26 $[ \\sum^{n-1}_{k=0} (26)^k]$ = $26 * \\frac{(26^n - 1)}{ 26 -1} $ = $26 * \\frac{(26^n - 1)}{ 25}$   \n",
    "<br>\n",
    "Let $V$ = $\\frac{26(26^n - 1)}{ 25}$  \n",
    "P(picking word correctly at first guess) = 1/V  \n",
    "P(picking word correctly at second guess) = V-1/V * 1/V-1 = 1/V  \n",
    "In the worst case, we would have to sample V times to get the correct next word. And following the above logic, P(picking word correctly at Vth guess) = 1/V \n",
    "$E(x) = \\sum^V_{x=1} x p(x)$ = $1/V \\sum^V_{x=1}x $ = = $\\frac{[1 + 2 + 3 + ... + V]}{V} = \\frac{V(V+1)}{2V} $  \n",
    "\n",
    "\n",
    "2. Entropy is lower in real world languages because they exhibit some kind of syntactic and semantic structrure. For eg. in English, a transitive verb would always followed by a direct object. This means after a verb, one can expect the next word to be of the following word classes: preposition or determiner or noun phrase  (e.g. Mat loves **me**/ Mat loves **a** good swim, etc.). Therefore, the presence of grammatical rules reduce the uncertainity of next words in real world languages. Language models (bigram, trigram, 4-gram) take context (previous n-1 words) into account to predict the nth word. Thus, instead of searching the entire vocabulary, only the words followed by previous n-1 words need to be considered. This reduces the search space and therefore, the uncertainity of guessing the correct word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bx1tnPQ4MgW"
   },
   "source": [
    "## Exercise 3: Kullback-Leibler Divergence (4 points)\n",
    "\n",
    "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$. \n",
    "Answer the following questions:\n",
    "\n",
    "#### 3.1. (0.5 points)\n",
    "\n",
    "How is $D_{KL}$ related to Cross-Entropy? Derive a mathematical expression that describes the relationship. \n",
    "\n",
    "\n",
    "#### 3.2. (0.5 points)\n",
    "\n",
    "Is minimizing $D_{KL}$ the same thing as minimizing Cross-Entropy?  Support your answer using your answer to 1.\n",
    "\n",
    "<!-- 3.3. Is $D_{KL}$ a distance metric, i. e. does $D_{KL}(P\\|Q) = D_{KL}(Q\\|P)$ hold? Justify you explanation by a proof or by a numerical counterexample. (1 point) -->\n",
    "\n",
    "\n",
    "#### 3.3 (3 points)\n",
    "\n",
    "For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
    "\n",
    "$\\forall x,y,z \\in U:$\n",
    "\n",
    "1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
    "2. $d(x,y) = d(y,x)$\n",
    "3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
    "\n",
    "Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
    "For each of the three points either prove that it holds for $K_{DL}$ or show a counterexample proving why it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** While cross-entropy gives the average number of **total bits** required to represent an event from a probability distribution Q instead of a probability distribution P, KL divergence gives the average number of **extra bits** to represent an event from Q instead of P.   \n",
    "$D(P || Q)$ = $– \\sum_{x\\in{X}} P(x) * \\log(\\frac{Q(x)}{P(x)})$  \n",
    "= $ – \\sum_{x\\in{X}} P(x) \\log Q(x) + \\sum_{x\\in{X}} P(x) \\log P(x)$\n",
    "= $E_P [-\\log(Q)] - E_P [-\\log(P)]$\n",
    "= $H(P,Q) - H(P)$ i.e. **cross entropy of Q from P - entropy of P** \n",
    "<br>\n",
    "\n",
    "**3.2** Yes, minimizing $D_{KL}$ is the same thing as minimizing $H(P,Q)$. From the equation in 3.1, we can rewrite cross entropy as the sum of entropy of the original distribution, P and the KL-divergence of the predicted distribution Q from the original distrbution P. i.e. $H(P,Q)$ =  $D(P || Q)$ + $H(P)$. This means to minimize $H(P,Q)$, either $D(P || Q)$ or $H(P)$ (or both) should be minimized. Since $H(P)$ denotes the entropy of the target/original distrubition, it cannot be minimized and hence, to minimize $H(P,Q)$, we need to minimize $KL(P || Q)$.\n",
    "\n",
    "**3.3**  $D_{KL}$ is not a distance metric because apart from the first property, the other two properties (2 and 3) do not hold.\n",
    " 1. $D(x || y)$ = $– \\sum_{i} x_i * \\log(\\frac{x_i}{y_i})$  \n",
    " when $x == y$,   \n",
    " $\\log(\\frac{x_i}{x_i}) = 0$ and thus, $D(x || x)$ = 0   \n",
    " \n",
    " 2. $k = \\{0,1\\}; p(0)=\\frac{1}{2}, q(0)=\\frac{1}{4}$  \n",
    " $D(x || y)$ = $x(0)\\log{\\frac{x(0)}{y(0)}} + x(1)\\log{\\frac{x(1)}{y(1)}}$ = $\\frac{1}{2}\\log{2} + \\frac{1}{2}\\log{\\frac{2}{3}}$ = **~0.25**  \n",
    " $D(y || x)$ = $y(0)\\log{\\frac{y(0)}{x(0)}} + y(1)\\log{\\frac{y(1)}{x(1)}}$ = $\\frac{1}{4}\\log{\\frac{1}{2}} + \\frac{3}{4}\\log{\\frac{3}{2}}$ = ** ~ -0.21 **  \n",
    " Thus, $D(x || y) \\ne D(y || x)$   \n",
    " \n",
    " 3. $D(x || z) \\nless D(x || y) + D(y || z)$  \n",
    " $k = \\{0,1\\}; x(0)=\\frac{1}{2}, y(0)=\\frac{1}{4}$, z(0)=$\\frac{1}{10}$  \n",
    " $D(x || z)$ =   $x(0)\\log{\\frac{x(0)}{z(0)}} + x(1)\\log{\\frac{x(1)}{z(1)}}$ = $\\frac{1}{2}\\log{5} + \\frac{1}{2}\\log{\\frac{5}{9}}$ = **~0.74**  \n",
    " $D(x || y)$ = $x(0)\\log{\\frac{x(0)}{y(0)}} + x(1)\\log{\\frac{x(1)}{y(1)}}$ = $\\frac{1}{2}\\log{2} + \\frac{1}{2}\\log{\\frac{2}{3}}$ = **~0.25**  \n",
    " $D(y || z)$ = $y(0)\\log{\\frac{y(0)}{z(0)}} + y(1)\\log{\\frac{y(1)}{z(1)}}$ = $\\frac{1}{4}\\log{\\frac{5}{2}} + \\frac{3}{4}\\log{\\frac{5}{6}}$ = ** ~ 0.14 **    \n",
    " Thus, $D(x || y) + D(y || z) = 0.25 + 0.14 = 0.39$ which is less than $D(x || z)$  = $0.74$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8zkUP3l4Mxw"
   },
   "source": [
    "## Bonus (1.5 points)\n",
    "\n",
    "1. Compute $D_{KL}(Q_1\\|P_1)$ for the following pair of sentences based on a unigram language model (word level).\n",
    "\n",
    "```\n",
    "p1: to be or not to be\n",
    "q1: to be or to be or not or to be be be\n",
    "```\n",
    "\n",
    " Do so by implementing the function `dkl` in `bonus.py`. You will also have to calculate the distributions $P_1$, $Q_1$; for this, you can either reuse your code from the last assignment or implement a new function in `bonus.py`. (1 point)\n",
    "\n",
    "2. Suppose the sentences in 1. would be replaced by the following sequences of symbols. You can imagine them to be sequences of nucleobases in a [coding](https://en.wikipedia.org/wiki/Coding_region) region of a gene in your genome.\n",
    "\n",
    "```\n",
    "p2: ACTGACACTGAC\n",
    "q2: ACTACTGACCCACTACTGACCC\n",
    "```\n",
    "\n",
    "Let $P_2$, $Q_2$ be the character-level unigram LMs derived from these sequences. What values will $D_{KL}(P_1\\|P_2)$, $D_{KL}(Q_1\\|Q_2)$ take? Does the quantity hold any information? Would computing $D_{KL}$ between distributions over two different natural languages hold any information? (0.5 points)\n",
    "\n",
    "No mathematical explanation nor coding required for the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9fBOuBNr6FY8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10020938467696769\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import bonus\n",
    "bonus = reload(bonus)\n",
    "\n",
    "P = bonus.find_ngram_probs(\"to be or not to be\")\n",
    "Q = bonus.find_ngram_probs(\"to be or to be or not or to be be be\")\n",
    "\n",
    "\n",
    "print(bonus.dkl(P,Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "2. $D_{KL}(P_1\\|P_2)$ =  $D_{KL}(Q_1\\|Q_2)$  = $\\infty$  \n",
    "No, computing $D_{KL}$ between distributions over two different natural languages would not hold any information because the distributions hold completely different states. Because these distributions are indepedent of each other, the quantity carries no information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
